import json
from datetime import datetime

from agents.questionPaperGeneratorAgent.questionPaperGenerator import QuestionPaperGenerator
text = """
Course: Programming Fundamentals and Data Structures

Course Outcomes:
CO1: Understand the basic concepts of programming, including variables, data types, and control structures.
CO2: Apply programming constructs such as loops, functions, and conditionals to solve computational problems.
CO3: Analyze and implement fundamental data structures to solve real-world problems efficiently.

Program Outcomes:
PO1: Engineering knowledge ‚Äì Apply knowledge of mathematics and computing fundamentals.
PO2: Problem analysis ‚Äì Identify, formulate, and analyze computational problems.
PO3: Design/development of solutions ‚Äì Design and implement efficient algorithms.

Syllabus Content:
Unit 1: Introduction to programming, variables, data types, input/output operations.
Unit 2: Control structures ‚Äì conditional statements, loops, and functions.
Unit 3: Arrays, strings, and basic operations.
Unit 4: Data structures ‚Äì stacks, queues, linked lists.
Unit 5: Searching and sorting algorithms, time and space complexity.


"""
qpgen = QuestionPaperGenerator(collectionName="test_ai_collection_v1")
output = qpgen.demoQuestionpaperGenerator(text=text,filePath="")
print("final output")
print(output)

# Save output to file
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_filename = f"question_paper_output_{timestamp}.json"

try:
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    print(f"\n‚úÖ Output saved to: {output_filename}")
except Exception as e:
    # If output is not JSON serializable, save as text
    output_filename = f"question_paper_output_{timestamp}.txt"
    with open(output_filename, 'w', encoding='utf-8') as f:
        f.write(str(output))
    print(f"\n‚úÖ Output saved to: {output_filename}")




# from services.prompt.promptProcessor import dataProcessor
# processor = dataProcessor()
# content = {
#     "co": [
#         "CO2: Analyze algorithm efficiency using time and space complexity."
#     ],
#     "po": [
#         "PO3: Design/development of solutions"
#     ]
# }

# questions = [
#     "What is time complexity?",
#     "Define space complexity."
# ]

# verdict = (
#     "The questions are too theoretical and focus on definitions. "
#     "At least one Analyze-level question is required. "
#     "Include problem-based questions comparing algorithm efficiency."
# )

# memory = {
#     "pastToolCalls": [
#         {
#             "tool": "question_generator",
#             "questions": [
#                 "Explain the time complexity of binary search and compare it with linear search."
#             ]
#         }
#     ],
#     "stepReasoning": [
#         "Generated one Analyze-level comparison question, but coverage is still insufficient"
#     ]
# }

# output = processor.questionEvaluatorMainLoop(
#     content,
#     questions,
#     verdict,
#     memory
# )

# print(output)

# input = """
# 1Ô∏è‚É£ What is Machine Learning?

# Machine Learning (ML) is a field of AI where systems learn patterns from data instead of being explicitly programmed.
# The model discovers relationships ‚Üí generalizes ‚Üí makes predictions.

# Key Types

# Supervised Learning ‚Äî learn from labeled data
# Examples: classification, regression

# Unsupervised Learning ‚Äî learn from unlabeled data
# Examples: clustering, dimensionality reduction

# Semi-Supervised Learning ‚Äî mix of labeled + unlabeled

# Reinforcement Learning ‚Äî learn by interacting with environment

# Self-Supervised Learning ‚Äî labels are generated from the data itself

# 2Ô∏è‚É£ Data in ML
# Dataset Components

# Features (X) ‚Äî inputs

# Target/Label (y) ‚Äî output

# Samples/Instances ‚Äî rows

# Feature Types

# Numerical ‚Äî continuous / discrete

# Categorical ‚Äî nominal / ordinal

# Text

# Image/audio/video time-series

# Dataset Split

# Training set ‚Äî learn parameters

# Validation set ‚Äî tune hyperparameters

# Test set ‚Äî evaluate final model

# Common split: 70% / 15% / 15% (or 80/20)

# 3Ô∏è‚É£ Data Pre-processing & Cleaning
# Handling Missing Values

# Delete rows (if few missing)

# Mean/median imputation

# Mode for categorical

# Advanced: KNN / iterative imputation

# Handling Outliers

# Z-score method

# IQR method

# Winsorization

# Log transform

# Feature Scaling

# Normalization (Min-Max)
# Good for neural networks

# Standardization (Z-score)
# Good for linear models & SVM

# Encoding Categorical Variables

# One-hot encoding

# Label encoding

# Target encoding

# Binary encoding

# Text Pre-processing

# Lowercasing

# Stopword removal

# Lemmatization/Stemming

# Tokenization

# Vectorization (TF-IDF, embeddings)

# 4Ô∏è‚É£ Feature Engineering
# Why?

# Improves model performance and interpretability.

# Techniques

# Polynomial features

# Interaction terms

# Domain-specific transformations

# Feature selection:

# Filter: correlation, chi-square

# Wrapper: RFE

# Embedded: LASSO, Tree importance

# Dimensionality reduction:

# PCA

# t-SNE

# UMAP

# 5Ô∏è‚É£ Supervised Learning Algorithms
# üìå Regression (predict continuous values)

# Linear Regression

# y = mx + c

# Minimizes Mean Squared Error (MSE)

# Regularized Regression

# Ridge ‚Äî L2 penalty

# Lasso ‚Äî L1 penalty

# Elastic Net ‚Äî L1 + L2

# Tree-based Regression

# Decision Trees

# Random Forest

# Gradient Boosting

# XGBoost / LightGBM / CatBoost

# üìå Classification (predict categories)

# Logistic Regression
# Outputs probability using sigmoid.

# K-Nearest Neighbors (KNN)
# Instance-based learning.

# Naive Bayes
# Uses Bayes‚Äô theorem + independence assumption.

# Decision Trees
# Splits based on information gain / Gini impurity.

# Ensembles

# Bagging ‚Äî Random Forest

# Boosting ‚Äî AdaBoost, GBM, XGBoost, LightGBM

# Stacking ‚Äî meta-model combining others

# Support Vector Machines (SVM)
# Finds separating hyperplane, uses kernels.

# Neural Networks
# Multiple layers learn complex functions.

# 6Ô∏è‚É£ Unsupervised Learning
# Clustering

# K-Means

# Hierarchical Clustering

# DBSCAN

# Gaussian Mixture Models

# Dimensionality Reduction

# PCA

# t-SNE

# UMAP

# Autoencoders

# Association Rule Learning

# Apriori

# FP-Growth

# 7Ô∏è‚É£ Evaluation Metrics
# Regression Metrics

# MAE ‚Äî Mean Absolute Error

# MSE ‚Äî Mean Squared Error

# RMSE ‚Äî Root MSE

# R¬≤ ‚Äî Variance explained

# MAPE ‚Äî Percentage error

# Classification Metrics

# Accuracy

# Precision

# Recall

# F1-Score

# ROC-AUC

# Log Loss

# Confusion Matrix
# 	Predicted +	Predicted ‚Äì
# Actual +	TP	FN
# Actual ‚Äì	FP	TN
# 8Ô∏è‚É£ Model Validation & Overfitting
# Bias-Variance Trade-off

# High bias ‚Üí underfitting

# High variance ‚Üí overfitting

# Prevent Overfitting

# Train-test split

# Cross-validation

# Regularization

# Early stopping

# Dropout (NNs)

# Simpler model

# Cross-Validation

# K-Fold (most common)

# Stratified K-Fold (classification)

# 9Ô∏è‚É£ Hyperparameter Tuning
# Methods

# Grid Search

# Random Search

# Bayesian Optimization

# Hyperband

# Genetic Algorithms

# Common Hyperparameters

# Learning rate

# Tree depth

# Number of trees

# Regularization strength

# Batch size (NNs)

# üîü Neural Networks & Deep Learning
# Basic Concepts

# Neuron = weighted sum + activation

# Layers:

# Input

# Hidden

# Output

# Activations

# ReLU

# Sigmoid

# Tanh

# Softmax

# Optimization

# Gradient Descent

# SGD

# Adam

# RMSProp

# Architectures

# CNNs ‚Üí images

# RNN/LSTM/GRU ‚Üí sequences

# Transformers ‚Üí NLP

# Autoencoders ‚Üí compression

# 1Ô∏è‚É£1Ô∏è‚É£ Reinforcement Learning
# Key Concepts

# Agent

# Environment

# Reward

# Policy

# Value function

# Algorithms

# Q-Learning

# Deep Q-Networks

# Policy Gradient

# 1Ô∏è‚É£2Ô∏è‚É£ ML Pipeline (End-to-End)

# Define problem

# Collect data

# Clean & preprocess

# EDA

# Feature engineering

# Split dataset

# Train model

# Tune hyperparameters

# Evaluate

# Deploy

# Monitor & retrain

# 1Ô∏è‚É£3Ô∏è‚É£ MLOps & Deployment
# Serving Methods

# REST API

# Batch inference

# Streaming inference

# Tools

# Flask / FastAPI

# Docker

# Kubernetes

# Airflow

# MLflow

# Kubeflow

# Monitoring

# Data drift

# Model drift

# Latency

# Accuracy decay

# 1Ô∏è‚É£4Ô∏è‚É£ Common ML Problems & Solutions
# Problem	Cause	Fix
# Overfitting	Complex model	Regularization, more data
# Underfitting	Too simple model	Add features, deeper model
# Data leakage	Using future info	Fix pipeline
# Imbalanced data	Skewed labels	SMOTE, class weights
# 1Ô∏è‚É£5Ô∏è‚É£ Ethics & Responsible AI

# Bias detection

# Fairness

# Transparency

# Privacy

# Explainability (SHAP, LIME)

# 1Ô∏è‚É£6Ô∏è‚É£ Important Concepts (Quick Reference)

# Curse of Dimensionality

# Cold start problem

# Bootstrapping

# Ensemble learning

# Gradient boosting vs bagging

# Cross-entropy loss

# Regularization (L1/L2)

# Feature importance

# ROC vs PR curves

# Stationarity in time-series

# Autocorrelation

# 1Ô∏è‚É£7Ô∏è‚É£ Time-Series ML (Brief)
# Techniques

# ARIMA / SARIMA

# Prophet

# LSTM

# Temporal CNN

# XGBoost on lag features

# Concepts

# Trend

# Seasonality

# Residuals

# Stationarity

# 1Ô∏è‚É£8Ô∏è‚É£ NLP Concepts

# Tokenization

# Stemming & Lemmatization

# TF-IDF

# Word2Vec & embeddings

# Transformers

# BERT/GPT

# Text classification

# Named Entity Recognition

# Sentiment analysis"""

# from agents.questionPaperGeneratorAgent.questionPaperGenerator import QuestionPaperGenerator
# qpgen = QuestionPaperGenerator(collectionName="test_ai_collection_v1")
# output = qpgen._recursiveChunker(input)
# base_metadata = {"CO": "CO1"}
# metadata_list = []
# for chunk in output:
#         meta = base_metadata.copy()
#         meta["text"] = chunk
#         metadata_list.append(meta)

# qpgen.ragSystem.process_and_store(chunks=output, metadata=metadata_list)
# content = {"co":"CO1"}
# generatedQuestions = ["what is ML","what is NLP"]
# verdict = "from notes add some other topics of Machine learning for four question"
# output = qpgen.mainQuestionPaperEvaluator(content=content,generatedQuestions=generatedQuestions,verdict=verdict)
# print(output)